{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Detection\n",
    "\n",
    "Attempting to finetune Vgg16 model to solve https://www.kaggle.com/c/state-farm-distracted-driver-detection.\n",
    "\n",
    "This notebook uses techniques described by the competition winner: https://www.kaggle.com/c/state-farm-distracted-driver-detection/discussion/22906."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib, sys\n",
    "sys.path.insert(0, './../../utils')\n",
    "import utils; importlib.reload(utils)\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "base_dir = '../'\n",
    "batch_size = 64\n",
    "dropout = 0.7\n",
    "\n",
    "start_w, start_h = 640, 480\n",
    "end_w, end_h = 224, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def large_bb_to_small(bb):\n",
    "    conv_x = (end_w / start_w)\n",
    "    conv_y = (end_h / start_h)\n",
    "    return [bb[0]*conv_x, bb[1]*conv_y, bb[2]*conv_x, bb[3]*conv_y]\n",
    "\n",
    "def small_bb_to_large(bb):\n",
    "    conv_x = (end_w / start_w)\n",
    "    conv_y = (end_h / start_h)\n",
    "    return [bb[0]/conv_x, bb[1]/conv_y, bb[2]/conv_x, bb[3]/conv_y]\n",
    "    \n",
    "def preprocess_img(img):\n",
    "    resized_image = to_keras(scipy.misc.imresize(img, (end_w, end_h)))\n",
    "    pred = dm.predict(cm.predict(np.array([resized_image])))[0]\n",
    "    pred = np.array(small_bb_to_large(pred)).astype(np.int32)\n",
    "    x = min(max(pred[0], 0), start_w-end_w)\n",
    "    y = min(max(pred[1], 0), start_h-end_h)\n",
    "    w = min(max(pred[2], end_w), start_w-x)\n",
    "    h = min(max(pred[3], end_h), start_h-y)\n",
    "    cropped_img = img[:, y:y+h, x:x+h]\n",
    "    return to_keras(scipy.misc.imresize(cropped_img, (end_w, end_h)))\n",
    "\n",
    "def gen_cropped_set(batches, dir_name):\n",
    "    filenames = batches.filenames\n",
    "    for f in tqdm(filenames):\n",
    "        path = base_dir + dir_name + f\n",
    "        img = Image.open(path)\n",
    "        img.load()\n",
    "        img = from_keras(preprocess_img(to_keras(np.asarray(img, dtype=\"int32\"))))\n",
    "        im = Image.fromarray(img)\n",
    "        im.save(base_dir + 'cropped_' + dir_name + f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "trn_batches = gen.flow_from_directory(base_dir + 'bb_train/', target_size=(end_h,end_w), shuffle=False)\n",
    "val_batches = gen.flow_from_directory(base_dir + 'bb_valid/', target_size=(end_h,end_w), shuffle=False)\n",
    "\n",
    "trn_filenames = [f.split('/')[-1] for f in trn_batches.filenames]\n",
    "val_filenames = [f.split('/')[-1] for f in val_batches.filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Annotated Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_json = {}\n",
    "with open(base_dir + 'trn_annotations.json') as file:\n",
    "    trn_json = json.load(file)\n",
    "    \n",
    "val_json = {}\n",
    "with open(base_dir + 'val_annotations.json') as file:\n",
    "    val_json = json.load(file)\n",
    "    \n",
    "trn_bbox = np.stack([large_bb_to_small(trn_json[f]) for f in trn_filenames]).astype(np.float32)\n",
    "val_bbox = np.stack([large_bb_to_small(val_json[f]) for f in val_filenames]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "trn_batches.reset()\n",
    "imgs = next(trn_batches)[0]\n",
    "show_bb(imgs[idx], trn_bbox[idx])\n",
    "\n",
    "val_batches.reset()\n",
    "imgs = next(val_batches)[0]\n",
    "show_bb(imgs[idx], val_bbox[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Pre-Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train/validation set pre-computing to speed up fine tuning\n",
    "cm = conv_model(vgg(output_size, dropout=dropout))\n",
    "\n",
    "trn_batches.reset()\n",
    "trn_features = cm.predict_generator(trn_batches, steps(trn_batches, batch_size//2), verbose=1)\n",
    "save_array(base_dir + 'models/bb_train_convlayer_features.bc', trn_features)\n",
    "\n",
    "val_batches.reset()\n",
    "val_features = cm.predict_generator(val_batches, steps(val_batches, batch_size//2), verbose=1)\n",
    "save_array(base_dir + 'models/bb_valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(base_dir + 'models/bb_train_convlayer_features.bc')\n",
    "val_features = load_array(base_dir + 'models/bb_valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg(output_size, dropout=dropout)\n",
    "model.load_weights(base_dir + 'models/aug_0.h5')\n",
    "model.pop()\n",
    "model.add(Dense(4))\n",
    "\n",
    "dm = vgg_fc_model(model, 4, dropout=dropout, output_activation=None)\n",
    "for i in range(len(dm.layers)): dm.layers[i].trainable = i >= len(dm.layers) - 3\n",
    "\n",
    "fit_with_features(dm, RMSprop(1e-3), 60, trn_features, trn_bbox, val_features, val_bbox, loss='mse')\n",
    "fit_with_features(dm, RMSprop(1e-4), 40, trn_features, trn_bbox, val_features, val_bbox, loss='mse')\n",
    "\n",
    "for l in dm.layers: l.trainable = True\n",
    "fit_with_features(dm, RMSprop(1e-3), 40, trn_features, trn_bbox, val_features, val_bbox, loss='mse')\n",
    "fit_with_features(dm, RMSprop(1e-4), 30, trn_features, trn_bbox, val_features, val_bbox, loss='mse')\n",
    "fit_with_features(dm, RMSprop(1e-5), 20, trn_features, trn_bbox, val_features, val_bbox, loss='mse')\n",
    "\n",
    "for l1,l2 in zip(model.layers[last_conv_idx(model) + 1:], dm.layers): l1.set_weights(l2.get_weights())\n",
    "model.save_weights(base_dir + 'models/bb0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Cropped Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg(output_size, dropout=dropout)\n",
    "model.pop()\n",
    "model.add(Dense(4))\n",
    "model.load_weights(base_dir + 'models/bb0.h5')\n",
    "dm = vgg_fc_model(model, 4, dropout=dropout, output_activation=None)\n",
    "cm = conv_model(vgg(output_size, dropout=dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "# Show bounding box\n",
    "val_batches.reset()\n",
    "img = next(val_batches)[0][idx]\n",
    "bb_pred = dm.predict(cm.predict(np.array([img])))[0]\n",
    "show_bb_pred(img, val_bbox[idx], bb_pred)\n",
    "\n",
    "# Show cropped image\n",
    "filenames = val_batches.filenames\n",
    "img = Image.open(base_dir + dir_name + '/' + filenames[idx])\n",
    "img.load()\n",
    "img = preprocess_img(to_keras(np.asarray(img, dtype=\"int32\")))\n",
    "plt.imshow(to_plot(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dir_name in ['valid/', 'train/', 'test/']:\n",
    "    batches = gen.flow_from_directory(base_dir + dir_name, target_size=(end_h,end_w), class_mode='categorical', shuffle=False, batch_size=batch_size)\n",
    "    gen_cropped_set(batches, dir_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
