{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Attempting to finetune Vgg16 model to solve https://www.kaggle.com/c/state-farm-distracted-driver-detection.\n",
    "\n",
    "This notebook uses techniques described by the competition winner: https://www.kaggle.com/c/state-farm-distracted-driver-detection/discussion/22906."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "import importlib, sys\n",
    "sys.path.insert(0, './../../utils')\n",
    "import utils; importlib.reload(utils)\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "# base_dir = '../sample/'\n",
    "base_dir = '../'\n",
    "batch_size = 64\n",
    "output_size = 10\n",
    "dropout = 0.8\n",
    "size = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17850 images belonging to 10 classes.\n",
      "Found 17850 images belonging to 10 classes.\n",
      "Found 4574 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "gen_with_aug = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "\n",
    "batches = gen.flow_from_directory(base_dir + 'cropped_train/', target_size=size, class_mode='categorical', shuffle=False, batch_size=batch_size)\n",
    "batches_aug = gen_with_aug.flow_from_directory(base_dir + 'cropped_train/', target_size=size, class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "val_batches = gen.flow_from_directory(base_dir + 'cropped_valid/', target_size=size, class_mode='categorical', shuffle=False, batch_size=batch_size)\n",
    "test_batches = gen.flow_from_directory(base_dir + 'cropped_test/', target_size=size, class_mode='categorical', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "trn_labels = to_categorical(batches.classes)\n",
    "val_labels = to_categorical(val_batches.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional pre-computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - 428s   \n",
      "72/72 [==============================] - 110s   \n"
     ]
    }
   ],
   "source": [
    "# Train/validation set pre-computing to speed up fine tuning\n",
    "m = vgg(output_size, dropout=dropout)\n",
    "cm = conv_model(m)\n",
    "\n",
    "batches.reset()\n",
    "trn_features = cm.predict_generator(batches, steps(batches, batch_size), verbose=1)\n",
    "save_array(base_dir + 'models/train_convlayer_features.bc', trn_features)\n",
    "\n",
    "val_batches.reset()\n",
    "val_features = cm.predict_generator(val_batches, steps(val_batches, batch_size), verbose=1)\n",
    "save_array(base_dir + 'models/valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test set convolutional features to compute nearest neighbors\n",
    "m = vgg(output_size, dropout=dropout)\n",
    "cm = conv_model(m)\n",
    "cm.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "test_batches.reset()\n",
    "test_features = cm.predict_generator(test_batches, steps(test_batches, batch_size), verbose=1)\n",
    "save_array(base_dir + 'models/test_convlayer_features.bc', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(base_dir + 'models/train_convlayer_features.bc')\n",
    "val_features = load_array(base_dir + 'models/valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ft_last(model, index):\n",
    "    dm = vgg_fc_model(model, output_size, dropout=dropout)\n",
    "    for i in range(len(dm.layers)): dm.layers[i].trainable = i >= len(dm.layers) - 3\n",
    "    \n",
    "    params = [[RMSProp(1e-3), 20], [RMSProp(1e-4), 20]]\n",
    "    fits_w_features(dm, params, trn_features, trn_labels, val_features, val_labels)\n",
    "    \n",
    "    for l1,l2 in zip(model.layers[last_conv_idx(model) + 1:], dm.layers): l1.set_weights(l2.get_weights())\n",
    "    model.save_weights(base_dir + 'models/bb_last' + str(index) + '.h5')\n",
    "    \n",
    "def ft_dense(model, index):\n",
    "    dm = vgg_fc_model(model, output_size, dropout=dropout)\n",
    "    for l in dm.layers: l.trainable = True\n",
    "    \n",
    "    params = [[RMSProp(1e-4), 20], [RMSProp(1e-5), 20]]\n",
    "    fits_w_features(dm, params, trn_features, trn_labels, val_features, val_labels)\n",
    "    \n",
    "    for l1,l2 in zip(model.layers[last_conv_idx(model) + 1:], dm.layers): l1.set_weights(l2.get_weights())\n",
    "    model.save_weights(base_dir + 'models/bb_dense' + str(index) + '.h5')\n",
    "    \n",
    "def ft_aug(model, index):\n",
    "    for i in range(len(model.layers)): model.layers[i].trainable = i >= 16\n",
    "    params = [[RMSProp(1e-5), 10], [RMSProp(1e-6), 10]]\n",
    "    for i in range(6): model_filename=base_dir + 'models/bb_aug' + str(index) + '.h5'\n",
    "    fits_w_batches(model, params, batches_aug, val_batches, model_filename=model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_logs()\n",
    "model = vgg(output_size, dropout=dropout)\n",
    "model.load_weights(base_dir + 'models/bb_dense0.h5')\n",
    "# ft_last(model, 0)\n",
    "# ft_dense(model, 0)\n",
    "ft_aug(model, 0)\n",
    "log_action(\"Finished training model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg(output_size, dropout=dropout)\n",
    "model.load_weights(base_dir + 'models/dense_pseudo_aug0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbor\n",
    "\n",
    "- Did not vary the number of neighbor (currently k = 10).\n",
    "- Did optimize the weight of neighbors in final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = load_array(base_dir + 'models/test_convlayer_features.bc')\n",
    "nsamples, nfilters, nx, ny = test_features.shape\n",
    "test_features = test_features.reshape((nsamples,nfilters*nx*ny))\n",
    "n=len(test_features)\n",
    "nb_neighbors = 10\n",
    "\n",
    "step = 40000\n",
    "result = []\n",
    "for i in tqdm(range(0, n, step)):\n",
    "    start_idx, end_idx = i, min(n, i+step)\n",
    "    nn = NearestNeighbors(nb_neighbors + 1, metric='cosine', algorithm='brute').fit(test_features)\n",
    "    dists, idxs = nn.kneighbors([test_features[j] for j in range(start_idx, end_idx)])\n",
    "    result += [np.vstack((idxs[i],dists[i])).T[1:] for i in range(end_idx-start_idx)]\n",
    "    \n",
    "save_array(base_dir + 'models/10_neighbors2.bc', np.array(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "- Did not visualize false positives, false negatives, most certain, most uncertain, etc.\n",
    "\n",
    "Main confusions seem to be:\n",
    "\n",
    "- c8 taken for c7 -> 162\n",
    "- c8 taken for c2 -> 83\n",
    "- c9 taken for c7 -> 68\n",
    "- c9 taken for c0 -> 44\n",
    "- c8 taken for c6 -> 36\n",
    "- c0 taken for c3 -> 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = np.array(all_preds[-1]).argmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(val_batches.classes, predicted_labels)\n",
    "\n",
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle\n",
    "\n",
    "- Did optimize clipping rate but did not play with asymetric clipping rates (different values for min and max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batches.reset()\n",
    "preds = model.predict_generator(test_batches, steps(test_batches, batch_size), verbose=1)\n",
    "save_array(base_dir + 'models/test_predictions.bc', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = load_array(base_dir + 'models/test_predictions.bc')\n",
    "nearest_neighbors = load_array(base_dir + 'models/10_neighbors2.bc')[:]\n",
    "weighted_preds = neighbor_averaged_preds(nearest_neighbors, preds)\n",
    "\n",
    "mx = 0.93\n",
    "clipped_preds = np.clip(weighted_preds, (1-mx)/9, mx)\n",
    "\n",
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "submission = pd.DataFrame(clipped_preds, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_batches.filenames])\n",
    "file_path = base_dir + 'submission.gz'\n",
    "submission.to_csv(file_path, index=False, compression='gzip')\n",
    "    \n",
    "from IPython.display import FileLink\n",
    "FileLink(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TO DO\n",
    "\n",
    "- Generate cropped images or use preprocessing\n",
    "- Train on cropped images\n",
    "- Check score with aug + neighbor average\n",
    "\n",
    "## Not tried\n",
    "\n",
    "- detector: fully convolutional NN w/ bigger images\n",
    "- detector: couple with category model to provide context\n",
    "- detector: crop only chest rather than entire body (tip of head, left of head, right of steering wheel, waist)\n",
    "- Categorical ensembling\n",
    "- Segment average (if the majority of similar images are certain, normalize all similar images to the same prediction)\n",
    "- Verify pseudo labeling efficiency on final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
